import json
import requests
from bs4 import BeautifulSoup
from langchain_upstage import ChatUpstage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import base64, json
from openai import OpenAI
import numpy as np
from typing import List, Dict

# 1. ê³µí†µ í•„ìš” í•¨ìˆ˜ ----------------------------------------------------------------------------------------------------------------

# ì´ë¯¸ì§€ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
# ì´ í•¨ìˆ˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì½ê³  base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë¬¸ìžì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
def encode_image_to_base64(path):
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

# ì´ë¯¸ì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
# ì´ í•¨ìˆ˜ëŠ” Upstage APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
def extract_information_from_image(API_KEY, file_path):
    # Encode the image
    encoded_image = encode_image_to_base64(file_path)
    
    # Define your schema (or use a pre-generated one)
    schema = {
        "type": "json_schema",
        "json_schema": {
            "name": "document_schema",
            "schema": {
                "type": "object",
                "properties": {
                    "í‚¤": {
                        "type": "number",
                        "description": "í‚¤ (cm)"
                    },
                    "ëª¸ë¬´ê²Œ": {
                        "type": "number",
                        "description": "ëª¸ë¬´ê²Œ (kg)"
                    },
                    "ì²´ì§ˆëŸ‰ì§€ìˆ˜": {
                        "type": "number",
                        "description": "ì²´ì§ˆëŸ‰ì§€ìˆ˜ (kg/ãŽ¡)"
                    },
                    "í—ˆë¦¬ë‘˜ë ˆ": {
                        "type": "number",
                        "description": "í—ˆë¦¬ë‘˜ë ˆ (cm)"
                    },
                    "í˜ˆì••": {
                        "type": "string",
                        "description": "í˜ˆì•• (ìˆ˜ì¶•ê¸°/ì´ì™„ê¸°, mmHg)"
                    },
                    "í˜ˆìƒ‰ì†Œ": {
                        "type": "string",
                        "description": "ë¹ˆí˜ˆ ë“± í˜ˆìƒ‰ì†Œ (g/dL)"
                    },
                    "ë¹ˆí˜ˆ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ë¹ˆí˜ˆ ì˜ì‹¬/ê¸°íƒ€ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ê³µë³µí˜ˆë‹¹": {
                        "type": "string",
                        "description": "ë‹¹ë‡¨ë³‘ ê³µë³µí˜ˆë‹¹ (mg/dL)"
                    },
                    "ë‹¹ë‡¨ë³‘ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ê³µë³µí˜ˆë‹¹ìž¥ì•  ì˜ì‹¬/ìœ ì§ˆí™˜ìž/ë‹¹ë‡¨ë³‘ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ì´ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ì´ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ê³ ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ê³ ë°€ë„ ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ì¤‘ì„±ì§€ë°©": {
                        "type": "string",
                        "description": "ì¤‘ì„±ì§€ë°© (mg/dL)"
                    },
                    "ì €ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ì €ë°€ë„ ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ì´ìƒì§€ì§ˆí˜ˆì¦ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ê³ ì½œë ˆìŠ¤í…Œë¡¤í˜ˆì¦ ì˜ì‹¬/ê³ ì¤‘ì„±ì§€ë°©í˜ˆì¦ ì˜ì‹¬/ë‚®ì€ HDL ì½œë ˆìŠ¤í…Œë¡¤ ì˜ì‹¬/ìœ ì§ˆí™˜ìž ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ": {
                        "type": "string",
                        "description": "í˜ˆì²­ í¬ë ˆì•„í‹°ë‹Œ (mg/dL)"
                    },
                    "eGFR": {
                        "type": "string",
                        "description": "ì‹ ì‚¬êµ¬ì²´ì—¬ê³¼ìœ¨ (mL/min/1.73ãŽ¡)"
                    },
                    "ì‹ ìž¥ì§ˆí™˜ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ì‹ ìž¥ê¸°ëŠ¥ ì´ìƒ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "AST": {
                        "type": "string",
                        "description": "AST (SGOT) (IU/L)"
                    },
                    "ALT": {
                        "type": "string",
                        "description": "ALT (SGPT) (IU/L)"
                    },
                    "ê°ë§ˆì§€í‹°í”¼": {
                        "type": "string",
                        "description": "ê°ë§ˆì§€í‹°í”¼ (Î³GTP) (IU/L)"
                    },
                    "ê°„ìž¥ì§ˆí™˜": {
                        "type": "string",
                        "description": "ì •ìƒ/ê°„ê¸°ëŠ¥ ì´ìƒ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ìš”ë‹¨ë°±": {
                        "type": "string",
                        "description": "ì •ìƒ/ê²½ê³„/ë‹¨ë°±ë‡¨ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "í‰ë¶€ì´¬ì˜": {
                        "type": "string",
                        "description": "ì •ìƒ/ë¹„í™œë™ì„± íê²°í•µ/íŠ¹ì • ì§ˆí™˜ ì˜ì‹¬/ê¸°íƒ€ ì†Œê²¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ê³¼ê±°ë³‘ë ¥": {
                        "type": "string",
                        "description": "ê³¼ê±°ë³‘ë ¥ (ì˜ˆ: ì—†ìŒ)"
                    },
                    "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥": {
                        "type": "string",
                        "description": "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥ (ì˜ˆ: ì—†ìŒ)"
                    }
                },
                "required": [
                "í‚¤",
                "ëª¸ë¬´ê²Œ",
                "ì²´ì§ˆëŸ‰ì§€ìˆ˜",
                "í—ˆë¦¬ë‘˜ë ˆ",
                "í˜ˆì••",
                "í˜ˆìƒ‰ì†Œ",
                "ë¹ˆí˜ˆ ì†Œê²¬",
                "ê³µë³µí˜ˆë‹¹",
                "ë‹¹ë‡¨ë³‘ ì†Œê²¬",
                "ì´ì½œë ˆìŠ¤í…Œë¡¤",
                "ê³ ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤",
                "ì¤‘ì„±ì§€ë°©",
                "ì €ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤",
                "ì´ìƒì§€ì§ˆí˜ˆì¦ ì†Œê²¬",
                "í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ",
                "eGFR",
                "ì‹ ìž¥ì§ˆí™˜ ì†Œê²¬",
                "AST",
                "ALT",
                "ê°ë§ˆì§€í‹°í”¼",
                "ê°„ìž¥ì§ˆí™˜",
                "ìš”ë‹¨ë°±",
                "í‰ë¶€ì´¬ì˜",
                "ê³¼ê±°ë³‘ë ¥",
                "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥"
                ]
            }
        }
    }
    
    # Define the API endpoint and headers
    # Create an API client
    extract_client = OpenAI(api_key=API_KEY, base_url="https://api.upstage.ai/v1/information-extraction")
    
    # Call the extraction API
    extraction_response = extract_client.chat.completions.create(
        model="information-extract",
        messages=[{"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encoded_image}"}}
        ]}],
        response_format=schema
    )
    
    # Parse the JSON result
    extracted_data = json.loads(extraction_response.choices[0].message.content)
    return extracted_data

# extract_information_from_image(API_KEY, file_path)

# upstage request ì˜¤ë¥˜ í™•ì¸
def print_upstage_error(response):
    if 'error' in response.json().keys():
        return response.json()['error']

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_function(API_KEY, file_path):

    url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {API_KEY}"}
    files = {"document": open(file_path, "rb")}
    data = {"ocr": "force", "base64_encoding": "['table']", "model": "document-parse"}
    response = requests.post(url, headers=headers, files=files, data=data)

    # upstage ì˜¤ë¥˜ í™•ì¸
    upstage_error = print_upstage_error(response)
    if upstage_error:
        return upstage_error
    
    # ë©”ì¸ ê¸°ëŠ¥
    contents_html = response.json()['content']['html']
    soup = BeautifulSoup(contents_html)
    text = soup.find(attrs = {'id': '20'}).text

    return text

# ê°œë°œìš© ì˜ˆì‹œ ë°ì´í„° ------------------------------------------------------------------------------------------------------------------

API_KEY = "" # í…ŒìŠ¤íŠ¸ìš© API í‚¤
file_path = "./data/test_image.jpg" # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ê²½ë¡œ

test_data = """
{
"ë‚˜ì´": 30ëŒ€,
"ë³„ëª…": ê±´ê°•í•œë‹¤ëžŒì¥,
"ì„±ë³„": ì—¬ì„±,
"í‚¤": 158,
"ëª¸ë¬´ê²Œ": 65.4,
"ì²´ì§ˆëŸ‰ì§€ìˆ˜": 26.2,
"í—ˆë¦¬ë‘˜ë ˆ": 86.0,
"í˜ˆì••": "135/88 mmHg",
"í˜ˆìƒ‰ì†Œ": "12.6",
"ë¹ˆí˜ˆ ì†Œê²¬": "ì •ìƒ",
"ê³µë³µí˜ˆë‹¹": "108",
"ë‹¹ë‡¨ë³‘ ì†Œê²¬": "ê³µë³µí˜ˆë‹¹ìž¥ì•  ì˜ì‹¬",
"ì´ì½œë ˆìŠ¤í…Œë¡¤": "198",
"ê³ ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": "55",
"ì¤‘ì„±ì§€ë°©": "140",
"ì €ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": "115",
"ì´ìƒì§€ì§ˆí˜ˆì¦ ì†Œê²¬": "ì •ìƒ",
"í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ": "1.5",
"eGFR": "48",
"ì‹ ìž¥ì§ˆí™˜ ì†Œê²¬": "ë§Œì„± ì‹ ìž¥ë³‘ ì˜ì‹¬",
"AST": "55",
"ALT": "62",
"ê°ë§ˆì§€í‹°í”¼": "85",
"ê°„ìž¥ì§ˆí™˜": "ì§€ì†ì  ê°„ê¸°ëŠ¥ ì´ìƒ",
"ìš”ë‹¨ë°±": "ì–‘ì„±(1+)",
"í‰ë¶€ì´¬ì˜": "ì •ìƒ"
}
"""

# 2. ê²€ì‚¬ ê²°ê³¼ ìš”ì•½ì„ ìœ„í•œ í•¨ìˆ˜ ----------------------------------------------------------------------------------------------------------------

def return_json_for_test(): # í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜

    return test_data

def return_json(API_KEY, file_path):
    # Instead of returning test data, use the real extraction function:
    return extract_information_from_image(API_KEY, file_path)

def return_summary_for_test(): #í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜

    temp = """
ðŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”, ê±´ê°•í•œë‹¤ëžŒì¥ë‹˜! ê²€ì‚¬ ê²°ê³¼ë¥¼ ì‚´íŽ´ë´¤ì–´ìš”. ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. í•¨ê»˜ ì°¨ê·¼ì°¨ê·¼ ì‚´íŽ´ë³´ë„ë¡ í•´ìš”.

ðŸ“Œ ì£¼ìš” ì‚¬í•­: í˜ˆë‹¹, ì‹ ìž¥ ê¸°ëŠ¥, ê°„ ê¸°ëŠ¥

ðŸ” ìžì„¸í•œ ì„¤ëª…:

* í˜ˆë‹¹: ê³µë³µ í˜ˆë‹¹ì´ ë†’ì€ íŽ¸ì´ì—ìš”. ì´ëŠ” í˜ˆë‹¹ì„ ì¡°ì ˆí•˜ëŠ” ë° ì£¼ì˜ê°€ í•„ìš”í•¨ì„ ì˜ë¯¸í•´ìš”. ê³¼ì¼, ì±„ì†Œì™€ ê°™ì€ ê±´ê°•í•œ íƒ„ìˆ˜í™”ë¬¼ì„ ì„ íƒí•˜ê³ , ê·œì¹™ì ì¸ ìš´ë™ì„ í†µí•´ í˜ˆë‹¹ì„ ê´€ë¦¬í•  ìˆ˜ ìžˆì–´ìš”.
(ìƒëžµ)

âœ… ìƒí™œìŠµê´€ íŒ:
(ìƒëžµ)

"""
    return temp

def return_summary(API_KEY, health_info):
    # Step 1: Define the conversation for Solar LLM using the provided prompt for an easy summary
    messages = [
        {
            "role": "system",
            "content": (
                "You are Dr. ì†Œë¼, a warm and friendly AI health coach.\n"
                "Your job is to gently explain a patient's health check-up results using soft and clear language.\n"
                "Focus only on what needs attention. Never use complex medical terms or diagnosis names.\n"
                "Explain in everyday language that is emotionally supportive and easy to understand."
            )
        },
        {
            "role": "user",
            "content": (
                f"Here is the health check-up result: {json.dumps(health_info, ensure_ascii=False)}\n"
                "Please provide an easy-to-understand summary focusing on key aspects that need attention."
            )
        }
    ]
    
    # Step 2: Call the Solar LLM using the Upstage API client
    client = OpenAI(api_key=API_KEY, base_url="https://api.upstage.ai/v1")
    response = client.chat.completions.create(
        model="solar-pro",
        messages=messages
    )
    
    # Extract the summary text from the response
    summary_text = response.choices[0].message['content']
    return summary_text

# Example usage
API_KEY = "your_api_key"
file_path = "path_to_your_file"
health_info = extract_information_from_image(API_KEY, file_path)  # Call the function first
summary = return_summary(API_KEY, health_info)  # Pass the result as a parameter
print(summary)

# 3. ì§„ë£Œê³¼ ì¶”ì²œ ----------------------------------------------------------------------------------------------------------------

# ì§„ë£Œê³¼ ì¶”ì²œ í•¨ìˆ˜ (ìž„ì‹œ)
def suggest_specialty(API_KEY, input_data):
    llm = ChatUpstage(api_key=API_KEY, model="solar-pro")
    
    prompt_template = """
ë‹¹ì‹ ì€ ì˜ë£Œ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ MAGICìž…ë‹ˆë‹¤. 
í™˜ìžì˜ ê±´ê°•ê²€ì§„ ê²°ê³¼ë¥¼ ë¶„ì„í•´, ë‹¤ìŒ ì„¸ ê°€ì§€ ì§„ë£Œê³¼ ì¤‘ ê°€ìž¥ ì ì ˆí•œ ê³³ì„ 1ê³³ ì¶”ì²œí•˜ê±°ë‚˜, ì¶”ì²œí•˜ì§€ ì•ŠìŒ:

1. ê°€ì •ì˜í•™ê³¼
2. ë‚´ê³¼
3. í˜ˆì•¡ë‚´ê³¼
4. í•´ë‹¹ì—†ìŒ

ì¶”ì²œ ê¸°ì¤€
ê°€ì •ì˜í•™ê³¼: ì²´ì¤‘ ì¦ê°€, ê²½ë¯¸í•˜ê±°ë‚˜ ê²½ê³„ì„± ìœ„í—˜ ì†Œê²¬, ë‹¨ì¼ ê±´ê°•ë¬¸ì œê°€ ì˜ì‹¬ë  ë•Œ, ë‹¤ì–‘í•œ ì§ˆí™˜ì´ ìžˆìœ¼ë‚˜ ë³µí•©ì ì´ì§€ ì•Šì€ ê²½ìš°
ë‚´ê³¼: ê³ í˜ˆì••, ë‹¹ë‡¨ ë“± ëª…í™•í•œ ì§ˆë³‘ ì§„ë‹¨ì´ ìžˆëŠ” ê²½ìš°, ì—¬ëŸ¬ ì§ˆí™˜ì´ ë³µí•©ì ìœ¼ë¡œ ì¡´ìž¬í•˜ëŠ” ê²½ìš° (ì˜ˆ: ê³ ì§€í˜ˆì¦ + ê³ í˜ˆì•• ë“±), ê°€ì •ì˜í•™ê³¼ì—ì„œ ë‹¤ë£¨ê¸° ì–´ë ¤ìš´ ë³µìž¡ë„ì¼ ê²½ìš°

í˜ˆì•¡ë‚´ê³¼: í˜ˆìƒ‰ì†Œ ìˆ˜ì¹˜ê°€ ë‚®ê±°ë‚˜ ë†’ì€ ê²½ìš° (ë¹ˆí˜ˆ ë˜ëŠ” ì í˜ˆêµ¬ ì¦ê°€ ë“±), ë°±í˜ˆêµ¬, í˜ˆì†ŒíŒ ìˆ˜ì¹˜ ì´ìƒ, ê¸°íƒ€ í˜ˆì•¡ ì´ìƒ ì†Œê²¬ì´ ìžˆì„ ë•Œ

ì¶”ê°€ ì§€ì¹¨
ê°„ê²°í•˜ê³  ëª…í™•í•œ ìš”ì•½ê³¼ í•¨ê»˜ ì§„ë£Œê³¼ë¥¼ ì¶”ì²œí•  ê²ƒ
ë™ì¼ í™˜ìžì—ê²Œ ë³µí•©ì ìœ¼ë¡œ ì—¬ëŸ¬ ì§ˆí™˜ì´ ë°œê²¬ë˜ì–´ë„, í˜¹ì€ íŒë‹¨ì´ ì• ë§¤í•œ ê²½ìš°ë¼ë„ ìœ„ ê¸°ì¤€ì„ í† ëŒ€ë¡œ ê°€ìž¥ ì í•©í•œ ê³¼ë¥¼ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì¶”ì²œí•  ê²ƒ
ê²€ì§„ ê²°ê³¼ê°€ ëª¨ë‘ ì •ìƒì´ë¼ë©´ "ì¶”ì²œ_ì§„ë£Œê³¼"ë¥¼ "í•´ë‹¹ì—†ìŒ"ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ 
ì˜ˆì‹œ ì¶œë ¥ê³¼ ê°™ì´ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ

ì˜ˆì‹œ: ì²´ì¤‘ ì¦ê°€, í˜ˆìƒ‰ì†Œ ìˆ˜ì¹˜ ì •ìƒ, ê³µë³µ í˜ˆë‹¹ ì•½ê°„ ë†’ìŒ (ê²½ê³„ ìˆ˜ì¤€)
ì˜ˆì‹œ ì¶œë ¥:
{
	"ê°„ëžµí•œ_ìš”ì•½": "ì²´ì¤‘ ì¦ê°€ì™€ ê²½ê³„ ìˆ˜ì¤€ì˜ í˜ˆë‹¹ì€ ë¹„êµì  ê²½ë¯¸í•œ ì†Œê²¬ì´ë©°, ë‹¨ì¼ ë¬¸ì œ ìœ„ì£¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë¯€ë¡œ ê°€ì •ì˜í•™ê³¼ê°€ ì í•©í•©ë‹ˆë‹¤.",
	"ì¶”ì²œ_ì§„ë£Œê³¼": "ê°€ì •ì˜í•™ê³¼"
}
    """
    final_prompt = PromptTemplate.from_template(prompt_template)
    output_parser = StrOutputParser()

    chain = final_prompt | llm | output_parser
    response = chain.invoke({"input_data": input_data})
    
    return response

# 4. RAG ë°”íƒ• ì˜ë£Œ comment----------------------------------------------------------------------------------------------------------------

class HealthRAGSystem:
    def __init__(self, api_key: str):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.upstage.ai/v1"
        )
        self.health_status = None
        self.guidelines_db = []  # This would be your vector database of medical guidelines
        
    def load_health_status(self, health_status: Dict):
        """Load the patient's health status"""
        self.health_status = health_status

    def preprocess_query(self, query: str) -> str:
        """Optional query preprocessing"""
        # For now, just return the original query
        # You can add keyword extraction or other preprocessing here
        return query

    def get_embeddings(self, text: str) -> np.ndarray:
        """Get embeddings using Solar API"""
        response = self.client.embeddings.create(
            model="embedding-query",  # Use appropriate embedding model
            input=text
        )
        return np.array(response.data[0].embedding)

    def retrieve_relevant_snippets(self, query: str, top_k: int = 3) -> List[str]:
        """Retrieve relevant guideline snippets using vector similarity"""
        query_embedding = self.get_embeddings(query)
        
        # Calculate similarities with all guidelines
        similarities = []
        for guideline in self.guidelines_db:
            guideline_embedding = self.get_embeddings(guideline)
            similarity = np.dot(query_embedding, guideline_embedding)
            similarities.append((similarity, guideline))
        
        # Sort by similarity and get top k
        similarities.sort(reverse=True)
        return [snippet for _, snippet in similarities[:top_k]]

    def construct_prompt(self, query: str, relevant_snippets: List[str]) -> str:
        """Construct the prompt for the LLM"""
        prompt = f"""[ì‚¬ìš©ìžì˜ ê±´ê°• ê²€ì§„ ê²°ê³¼]ë¥¼ ì—¼ë‘ì— ë‘ë©´ì„œ [ê´€ë ¨ ì˜ë£Œ ì§€ì¹¨]ì„ ë°”íƒ•ìœ¼ë¡œ [ì‚¬ìš©ìž ì§ˆë¬¸]ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì§€ì¹¨ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. 
        #ì‚¬ìš©ìžì˜ ê±´ê°• ê²€ì§„ ê²°ê³¼:{json.dumps(self.health_status, indent=2, ensure_ascii=False)}

        #ê´€ë ¨ ì˜ë£Œ ì§€ì¹¨:
        """
        for i, snippet in enumerate(relevant_snippets, 1):
            prompt += f"{i}. {snippet}\n"

        prompt += f"\n#ì‚¬ìš©ìž ì§ˆë¬¸: {query}\n"
        
        return prompt

    def generate_response(self, query: str) -> str:
        """Generate final response using Solar LLM"""
        # Preprocess query
        processed_query = self.preprocess_query(query)
        
        # Retrieve relevant snippets
        relevant_snippets = self.retrieve_relevant_snippets(processed_query)
        
        # Construct prompt
        prompt = self.construct_prompt(processed_query, relevant_snippets)
        
        # Generate response using Solar
        response = self.client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content