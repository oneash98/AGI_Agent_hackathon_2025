import json
import requests
from bs4 import BeautifulSoup
from langchain_upstage import ChatUpstage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import base64, json
from openai import OpenAI
from scipy.spatial import KDTree
import numpy as np

def encode_image_to_base64(path):
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

def return_json(API_KEY, file_path, result_queue):
    schema_client = OpenAI(api_key=API_KEY, base_url="https://api.upstage.ai/v1/information-extraction/schema-generation")
    # Encode the image
    encoded_image = encode_image_to_base64(file_path)
    
    schema_response = schema_client.chat.completions.create(
        model="information-extract",
        messages=[{"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encoded_image}"}}
        ]}],
    )
    # Define your schema (or use a pre-generated one)
    schema = {
        "type": "json_schema",
        "json_schema": {
            "name": "document_schema",
            "schema": {
                "type": "object",
                "properties": {
                    "í‚¤": {
                        "type": "number",
                        "description": "í‚¤ (cm)"
                    },
                    "ëª¸ë¬´ê²Œ": {
                        "type": "number",
                        "description": "ëª¸ë¬´ê²Œ (kg)"
                    },
                    "ì²´ì§ˆëŸ‰ì§€ìˆ˜": {
                        "type": "number",
                        "description": "ì²´ì§ˆëŸ‰ì§€ìˆ˜ (kg/ã¡)"
                    },
                    "í—ˆë¦¬ë‘˜ë ˆ": {
                        "type": "number",
                        "description": "í—ˆë¦¬ë‘˜ë ˆ (cm)"
                    },
                    "í˜ˆì••": {
                        "type": "string",
                        "description": "í˜ˆì•• (ìˆ˜ì¶•ê¸°/ì´ì™„ê¸°, mmHg)"
                    },
                    "í˜ˆìƒ‰ì†Œ": {
                        "type": "string",
                        "description": "ë¹ˆí˜ˆ ë“± í˜ˆìƒ‰ì†Œ (g/dL)"
                    },
                    "ë¹ˆí˜ˆ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ë¹ˆí˜ˆ ì˜ì‹¬/ê¸°íƒ€ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ê³µë³µí˜ˆë‹¹": {
                        "type": "string",
                        "description": "ë‹¹ë‡¨ë³‘ ê³µë³µí˜ˆë‹¹ (mg/dL)"
                    },
                    "ë‹¹ë‡¨ë³‘ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ê³µë³µí˜ˆë‹¹ì¥ì•  ì˜ì‹¬/ìœ ì§ˆí™˜ì/ë‹¹ë‡¨ë³‘ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ì´ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ì´ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ê³ ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ê³ ë°€ë„ ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ì¤‘ì„±ì§€ë°©": {
                        "type": "string",
                        "description": "ì¤‘ì„±ì§€ë°© (mg/dL)"
                    },
                    "ì €ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤": {
                        "type": "string",
                        "description": "ì €ë°€ë„ ì½œë ˆìŠ¤í…Œë¡¤ (mg/dL)"
                    },
                    "ì´ìƒì§€ì§ˆí˜ˆì¦ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ê³ ì½œë ˆìŠ¤í…Œë¡¤í˜ˆì¦ ì˜ì‹¬/ê³ ì¤‘ì„±ì§€ë°©í˜ˆì¦ ì˜ì‹¬/ë‚®ì€ HDL ì½œë ˆìŠ¤í…Œë¡¤ ì˜ì‹¬/ìœ ì§ˆí™˜ì ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ": {
                        "type": "string",
                        "description": "í˜ˆì²­ í¬ë ˆì•„í‹°ë‹Œ (mg/dL)"
                    },
                    "eGFR": {
                        "type": "string",
                        "description": "ì‹ ì‚¬êµ¬ì²´ì—¬ê³¼ìœ¨ (mL/min/1.73ã¡)"
                    },
                    "ì‹ ì¥ì§ˆí™˜ ì†Œê²¬": {
                        "type": "string",
                        "description": "ì •ìƒ/ì‹ ì¥ê¸°ëŠ¥ ì´ìƒ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "AST": {
                        "type": "string",
                        "description": "AST (SGOT) (IU/L)"
                    },
                    "ALT": {
                        "type": "string",
                        "description": "ALT (SGPT) (IU/L)"
                    },
                    "ê°ë§ˆì§€í‹°í”¼": {
                        "type": "string",
                        "description": "ê°ë§ˆì§€í‹°í”¼ (Î³GTP) (IU/L)"
                    },
                    "ê°„ì¥ì§ˆí™˜": {
                        "type": "string",
                        "description": "ì •ìƒ/ê°„ê¸°ëŠ¥ ì´ìƒ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ìš”ë‹¨ë°±": {
                        "type": "string",
                        "description": "ì •ìƒ/ê²½ê³„/ë‹¨ë°±ë‡¨ ì˜ì‹¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "í‰ë¶€ì´¬ì˜": {
                        "type": "string",
                        "description": "ì •ìƒ/ë¹„í™œë™ì„± íê²°í•µ/íŠ¹ì • ì§ˆí™˜ ì˜ì‹¬/ê¸°íƒ€ ì†Œê²¬ ì¤‘ ì²´í¬ëœ í•­ëª©"
                    },
                    "ê³¼ê±°ë³‘ë ¥": {
                        "type": "string",
                        "description": "ê³¼ê±°ë³‘ë ¥ (ì˜ˆ: ì—†ìŒ)"
                    },
                    "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥": {
                        "type": "string",
                        "description": "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥ (ì˜ˆ: ì—†ìŒ)"
                    }
                },
                "required": [
                "í‚¤",
                "ëª¸ë¬´ê²Œ",
                "ì²´ì§ˆëŸ‰ì§€ìˆ˜",
                "í—ˆë¦¬ë‘˜ë ˆ",
                "í˜ˆì••",
                "í˜ˆìƒ‰ì†Œ",
                "ë¹ˆí˜ˆ ì†Œê²¬",
                "ê³µë³µí˜ˆë‹¹",
                "ë‹¹ë‡¨ë³‘ ì†Œê²¬",
                "ì´ì½œë ˆìŠ¤í…Œë¡¤",
                "ê³ ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤",
                "ì¤‘ì„±ì§€ë°©",
                "ì €ë°€ë„ì½œë ˆìŠ¤í…Œë¡¤",
                "ì´ìƒì§€ì§ˆí˜ˆì¦ ì†Œê²¬",
                "í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ",
                "eGFR",
                "ì‹ ì¥ì§ˆí™˜ ì†Œê²¬",
                "AST",
                "ALT",
                "ê°ë§ˆì§€í‹°í”¼",
                "ê°„ì¥ì§ˆí™˜",
                "ìš”ë‹¨ë°±",
                "í‰ë¶€ì´¬ì˜",
                "ê³¼ê±°ë³‘ë ¥",
                "ì•½ë¬¼ì¹˜ë£Œë³‘ë ¥"
                ]
            }
        }
    }
    
    # --- Step 2: Extract Information ---
    extract_client = OpenAI(api_key=API_KEY, base_url="https://api.upstage.ai/v1/information-extraction")
    extraction_response = extract_client.chat.completions.create(
        model="information-extract",
        messages=[{"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encoded_image}"}}
        ]}],
        response_format=schema  # Use the auto-generated schema
    )

    # --- Step 3: Print or process the result ---
    extracted_data = json.loads(extraction_response.choices[0].message.content)
    result_queue.put(("health_info", extracted_data))
    return extracted_data

# upstage request ì˜¤ë¥˜ í™•ì¸
def print_upstage_error(response):
    if 'error' in response.json().keys():
        return response.json()['error']

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_function(API_KEY, file_path):

    url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {API_KEY}"}
    files = {"document": open(file_path, "rb")}
    data = {"ocr": "force", "base64_encoding": "['table']", "model": "document-parse"}
    response = requests.post(url, headers=headers, files=files, data=data)

    # upstage ì˜¤ë¥˜ í™•ì¸
    upstage_error = print_upstage_error(response)
    if upstage_error:
        return upstage_error
    
    # ë©”ì¸ ê¸°ëŠ¥
    contents_html = response.json()['content']['html']
    soup = BeautifulSoup(contents_html)
    text = soup.find(attrs = {'id': '20'}).text

    return text


def return_summary_for_test():

    temp = "ìš”ì•½"

    return temp


def return_simple_explanation(API_KEY, health_info, result_queue):
    # Step 1 Define the conversation for Solar LLM using the provided prompt for an easy summary
    messages = [
        {
            "role": "system",
            "content": (
                """You are MAGIC, a Korean, warm and friendly AI health coach.
                Your job is to gently explain a patient's health check-up results using friendly language.
                Focus only on what needs attention. Never use complex medical terms or diagnosis names.
                Explain in everyday language that is emotionally supportive and easy to understand.
                Use emojis to make the explanation more friendly and engaging.

                Input:

                Doctor's health check-up summary (written in Korean)
                Patient nickname (e.g., ìŠ¬ê¸°ë¡œìš´ê³ ì–‘ì´)
                Age group (e.g., 30ëŒ€), Gender (e.g., ì—¬ì„±)
                Output Format:
                {
                    ğŸ‘‹ Greeting & Empathy (1 short paragraph)
                    Greet the patient using their nickname. Briefly mention youâ€™ve read their results and will explain gently.

                    ğŸ“Œ Health Summary (2â€“3 sentences max)
                    Summarize the main areas that need attention. Keep it short and focused.

                    ğŸ” Detailed Explanation (up to 3 areas)
                    For each issue:

                    What was found (in natural words)
                    Why it matters (soft explanation)
                    How to help the body (adjust food, exercise, or habits)
                    Use lifestyle examples that are appropriate to age/gender.
                    âœ… Lifestyle Tips (2â€“3 total)
                    MUST Consider patient's age and sex
                    Offer kind and simple suggestions on food, movement, or daily routines.

                    Examples should suit the patientâ€™s profile (age and sex):
                    Young woman â†’ ë–¡ë³¶ì´, ë§ˆë¼íƒ•, í™ˆíŠ¸
                    Older woman â†’ ë°˜ì°¬, ì‚°ì±…, ìš”ê°€
                    Young man â†’ ì¹˜í‚¨, ë¼ë©´, í—¬ìŠ¤
                    Older man â†’ ë“±ì‚°, ìœ ì‚°ì†Œ ìš´ë™
                    ğŸ’¬ Friendly Encouragement
                    End with comforting words to support the patient and encourage action.
                }
                """
            )
        },
        {
            "role": "user",
            "content": (
                #f"Patient's nickname: {health_info['ë³„ëª…']}\n   "
                #f"Patient's age group: {health_info['ë‚˜ì´']}\n"
                #f"Patient's gender: {health_info['ì„±ë³„']}\n"
                f"Health check-up result: {json.dumps(health_info, ensure_ascii=False)}\n"
                #f"Brief summary of the health check-up result: {summary_professional}\n"

                "Please provide an easy-to-understand summary focusing on key aspects that need attention."
            )
        }
    ]
    
    try:
        # Step 2: Call the Solar LLM using the Upstage API client
        client = OpenAI(api_key=API_KEY, base_url="https://api.upstage.ai/v1")
        response = client.chat.completions.create(
            model="solar-pro",
            messages=messages
        )
        
        # Step 3: Extract the summary text from the response
        summary_text = response.choices[0].message.content
        result_queue.put(("simple_explanation", summary_text))
        return summary_text
    
    except Exception as e:
        # Return a fallback message if the API call fails
        print(f"Error in return_summary: {str(e)}")
        error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì•½ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        result_queue.put(("explanation_error", error_message))
        return error_message


# ì§„ë£Œê³¼ ì¶”ì²œ í•¨ìˆ˜ (ì¶”ì²œ ì‚¬ìœ ì™€ ì¶”ì²œ ì§„ë£Œê³¼ ë°˜í™˜)
def suggest_specialty(API_KEY, health_info, summary, result_queue):
    llm = ChatUpstage(api_key=API_KEY, model="solar-pro")
    
    prompt_template = """
    ë‹¹ì‹ ì€ ì˜ë£Œ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ MAGICì…ë‹ˆë‹¤. 
    í™˜ìì˜ <ê±´ê°•ê²€ì§„ ê²°ê³¼>ì™€ <ìš”ì•½ ì •ë³´>ë¥¼ ë¶„ì„í•´, ë‹¤ìŒ ì§„ë£Œê³¼ ì¤‘ ê°€ì¥ ì ì ˆí•œ ê³³ì„ 1ê³³ ì¶”ì²œí•˜ê±°ë‚˜, ì¶”ì²œí•˜ì§€ ì•ŠìŒ:
    
    - ê°€ì •ì˜í•™ê³¼
    - ë‚´ê³¼
    - í˜ˆì•¡ë‚´ê³¼
    - ì†Œí™”ê¸°ë‚´ê³¼
    - ë‚´ë¶„ë¹„ë‚´ê³¼
    - ì‹¬ì¥ë‚´ê³¼
    - ì‹ ì¥ë‚´ê³¼
    - í•´ë‹¹ì—†ìŒ
    
    ì¶”ì²œ ê¸°ì¤€
    * í•´ë‹¹ì—†ìŒ: ëª¨ë“  ê²ƒì´ ì •ìƒì¸ ê²½ìš° ì¶”ì²œí•œë‹¤.
    * ê°€ì •ì˜í•™ê³¼: ì²´ì¤‘ ì¦ê°€, ê²½ë¯¸í•˜ê±°ë‚˜ ê²½ê³„ì„± ìœ„í—˜ ì†Œê²¬, ë‹¨ì¼ ê±´ê°•ë¬¸ì œê°€ ì˜ì‹¬ë  ë•Œ, ë‹¤ì–‘í•œ ì§ˆí™˜ì´ ìˆìœ¼ë‚˜ ë³µí•©ì ì´ì§€ ì•Šì€ ê²½ìš°
    * ë‚´ê³¼: 
    	- í˜ˆì•¡ë‚´ê³¼: í˜ˆìƒ‰ì†Œ ìˆ˜ì¹˜ê°€ ë‚®ê±°ë‚˜ ë†’ì€ ê²½ìš° (ë¹ˆí˜ˆ ë˜ëŠ” ì í˜ˆêµ¬ ì¦ê°€ ë“±), ë°±í˜ˆêµ¬, í˜ˆì†ŒíŒ ìˆ˜ì¹˜ ì´ìƒ, ê¸°íƒ€ í˜ˆì•¡ ì´ìƒ ì†Œê²¬ì´ ìˆì„ ë•Œ
    	- ì‹ ì¥ë‚´ê³¼: ì‹ ì¥ê¸°ëŠ¥ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° (GFR ë‚®ìŒ, í¬ë ˆì•„ë‹Œ ìƒìŠ¹ ë“±)
    	- ì‹¬ì¥ë‚´ê³¼: ê³ í˜ˆì••ì¸ ê²½ìš° ìš°ì„ , ë˜ëŠ” í˜ˆì••ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° (ìˆ˜ì¶•ê¸° í˜ˆì•• ë˜ëŠ” ì´ì™„ê¸° í˜ˆì•• ë†’ìŒ, ìˆ˜ì¶•ê¸° í˜ˆì••ê³¼ ì´ì™„ê¸° í˜ˆì•• ì°¨ì´ê°€ í¼)
    	- ë‚´ë¶„ë¹„ë‚´ê³¼: í˜ˆë‹¹ì— ì´ìƒì´ ìˆëŠ” ê²½ìš°, í˜¹ì€ ì´ìƒì§€ì§ˆí˜ˆì¦ê³¼ í˜ˆë‹¹ì— ê²½ê³„ì„± ì˜ì‹¬ì†Œê²¬ì´ ìˆëŠ” ê²½ìš°, ì—¬ëŸ¬ ë‚´ë¶„ë¹„ ì§ˆí™˜ì´ ë³µí•©ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ì˜ˆ: ë‹¹ë‡¨ + ê³ ì§€í˜ˆì¦ + ê³ í˜ˆì•• ë“±), ê°€ì •ì˜í•™ê³¼ì—ì„œ ë‹¤ë£¨ê¸° ì–´ë ¤ìš´ ë³µì¡ë„ì¼ ê²½ìš°
    	- ì†Œí™”ê¸°ë‚´ê³¼: ê°„ê¸°ëŠ¥ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° (ALT, AST, ì§€í‹°í”¼ ìƒìŠ¹ ë“±)
    	- ë‚´ê³¼: ì§ˆí™˜ì˜ ì˜ì‹¬ë˜ì§€ë§Œ, ì•ì„  ì§ˆí™˜ë“¤ì´ ì•„ë‹Œ ê²½ìš°.
    
    ì¶”ê°€ ì§€ì¹¨
    ê°„ê²°í•˜ê³  ëª…í™•í•œ ìš”ì•½ê³¼ í•¨ê»˜ ì§„ë£Œê³¼ë¥¼ ì¶”ì²œí•  ê²ƒ
    ë™ì¼ í™˜ìì—ê²Œ ë³µí•©ì ìœ¼ë¡œ ì—¬ëŸ¬ ì§ˆí™˜ì´ ë°œê²¬ë˜ì–´ë„, í˜¹ì€ íŒë‹¨ì´ ì• ë§¤í•œ ê²½ìš°ë¼ë„ ìœ„ ê¸°ì¤€ì„ í† ëŒ€ë¡œ ê°€ì¥ ì í•©í•œ ê³¼ë¥¼ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì¶”ì²œí•  ê²ƒ
    ê²€ì§„ ê²°ê³¼ê°€ ëª¨ë‘ ì •ìƒì´ë¼ë©´ "ì¶”ì²œ_ì§„ë£Œê³¼"ë¥¼ "í•´ë‹¹ì—†ìŒ"ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ 
    ì˜ˆì‹œ ì¶œë ¥ê³¼ ê°™ì´ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ
    
    ì˜ˆì‹œ: ì²´ì¤‘ ì¦ê°€, í˜ˆìƒ‰ì†Œ ìˆ˜ì¹˜ ì •ìƒ, ê³µë³µ í˜ˆë‹¹ ì•½ê°„ ë†’ìŒ (ê²½ê³„ ìˆ˜ì¤€)
    ì˜ˆì‹œ ì¶œë ¥:
    {{
    	"ì¶”ì²œ_ì‚¬ìœ ": "ì²´ì¤‘ ì¦ê°€ì™€ ê²½ê³„ ìˆ˜ì¤€ì˜ í˜ˆë‹¹ì€ ë¹„êµì  ê²½ë¯¸í•œ ì†Œê²¬ì´ë©°, ë‹¨ì¼ ë¬¸ì œ ìœ„ì£¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë¯€ë¡œ ê°€ì •ì˜í•™ê³¼ê°€ ì í•©í•©ë‹ˆë‹¤.",
    	"ì¶”ì²œ_ì§„ë£Œê³¼": "ê°€ì •ì˜í•™ê³¼"
    }}
    
    <ê±´ê°•ê²€ì§„ ê²°ê³¼>:
    {health_info}

    <ìš”ì•½ ì •ë³´>:
    {summary}
    """
    final_prompt = PromptTemplate.from_template(prompt_template)
    output_parser = StrOutputParser()

    chain = final_prompt | llm | output_parser
    response = chain.invoke({"health_info": health_info, "summary": summary})

    temp = json.loads(response)
    reason = temp['ì¶”ì²œ_ì‚¬ìœ ']
    specialty = temp['ì¶”ì²œ_ì§„ë£Œê³¼']
    
    result_queue.put(("reason", reason))
    result_queue.put(("specialty", specialty))
    return reason, specialty



# ê°€ê¹Œìš´ ë³‘ì› ì°¾ê¸°
def get_nearest_clinics(clinics_info, longitude, latitude, specialty, k):
    if specialty in ('ê°€ì •ì˜í•™ê³¼', 'ë‚´ê³¼'):
        df = clinics_info[clinics_info['ì§„ë£Œê³¼'] == specialty]
    else:
        df = clinics_info[clinics_info[specialty] == 1]
    
    coords = df[['ì¢Œí‘œ(X)', 'ì¢Œí‘œ(Y)']].values
    tree = KDTree(coords)
    target = np.array([longitude, latitude])
    distance, indicies = tree.query(target, k=k)
    
    return df.iloc[indicies] # df ë°˜í™˜ -> UI íŒŒì¼ì—ì„œ í™”ë©´ì— í‘œì‹œ
